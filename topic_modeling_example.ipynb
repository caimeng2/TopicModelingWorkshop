{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Topic Modeling for Exploratory Text Analysis</center>\n",
    "<center>An SSDA workshop</center>\n",
    "<center>If you have any questions, feel free to contact me (Meng) at caimeng2@msu.edu <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we will be using a few packages that are not included in Anaconda by default. Please make sure to install `nltk`, `scikit-learn`, `spacy`, `pyLDAvis`, and `gensim`. The following commands should work for most systems. Detailed installation instructions could be found at each package's website (see links below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `conda install nltk` https://www.nltk.org/install.html\n",
    "\n",
    "- `conda install scikit-learn` https://scikit-learn.org/stable/install.html\n",
    "\n",
    "- `pip install spacy` https://spacy.io/usage\n",
    "\n",
    "- `pip install pyLDAvis` https://pypi.org/project/pyLDAvis/\n",
    "\n",
    "- `pip install gensim` https://radimrehurek.com/gensim/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already covered what LDA topic modeling is and why it is useful. In this notebook, we will take a look at how to build a topic model in Python.\n",
    "\n",
    "- [Step 1: data cleaning](#1)\n",
    "- [Step 2: tokenization](#2)\n",
    "- [Step 3: stemming / lemmatization](#3)\n",
    "- [Step 4: remove stop words](#4)\n",
    "\n",
    "At this point, the data is ready for topic modeling.\n",
    "\n",
    "- [Step 5: find topics](#5)\n",
    "- [Step 6: interpret the topics and improve the model](#6)\n",
    "\n",
    "Optional steps:\n",
    "\n",
    "- [Label documents](#7)\n",
    "- [Visualization](#8)\n",
    "\n",
    "Jump to:\n",
    "- [Activities - try it yourself](#activity)\n",
    "- [Additional resources](#additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:06:07.023327Z",
     "start_time": "2020-06-10T20:06:03.666825Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en import English\n",
    "from nltk.corpus import wordnet as wn\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis.gensim\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "pd.set_option(\"display.max_colwidth\", 80)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the above cell will take a few seconds to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "\n",
    "## Step 1: data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data: `msutweets.csv`. Three years' tweets from [@michiganstateu](https://twitter.com/michiganstateu) (from 4/30/2018 to 4/30/2020). No retweets.\n",
    "\n",
    "The whole dataset has 1580 tweets. Let's randomly draw a sample of 200 as our example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:08:18.998921Z",
     "start_time": "2020-06-10T20:08:18.958636Z"
    }
   },
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"msutweets.csv\", header=None) # import data\n",
    "raw.columns = [\"created_at\", \"id\", \"hashtags\", \"user_mentions\", \"in_reply_to_status_id\",\n",
    "                   \"in_reply_to_user_id\", \"in_reply_to_screen_name\", \"username\", \"id\",\n",
    "                   \"profile_location\", \"description\", \"user_url\", \"followers\", \"friends\",\n",
    "                   \"user_created_at\", \"verified\", \"geo\", \"coordinates\", \"place\",\n",
    "                   \"retweet\", \"favorite\", \"tweets\"] # set up column names\n",
    "df = raw[[\"tweets\"]] # remove columns we don't need\n",
    "df = df.sample(n=200, random_state=0) # randomly sample 200 tweets and set a seed for reproducibility\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **regular expressions** (also referred to as regex or regexp) to clean our text. This step usually requires customized solutions for different datasets. For tweets, generally, we would like to remove URLs, #hashtags, numbers, and punctuations. For our example, because the purpose is not to build a social network or do some sentiment analysis, we delete @mentions and emojis as well.\n",
    "\n",
    "On a side note, regular expression is a powerful tool for pattern matching in text. However, the syntax is hard to read and write. I found https://regex101.com/ very helpful for experimenting with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:10:37.482558Z",
     "start_time": "2020-06-10T20:10:37.474814Z"
    }
   },
   "outputs": [],
   "source": [
    "def TextCleaner(dirty_text):\n",
    "    \"\"\"Remove links, special characters, and numbers from text.\"\"\"\n",
    "    dirty_text = dirty_text.replace(\"&amp\", \"\") # remove \"&\"\n",
    "    semi_clean_text = \" \".join(re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\n",
    "                                      \"\", dirty_text).split()) # remove @somebody, special characters, and links\n",
    "    clean_text = \" \".join(re.sub(r\"\\d+\", \"\", semi_clean_text).split()) # remove numbers\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:10:38.569301Z",
     "start_time": "2020-06-10T20:10:38.556684Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_tweets = []\n",
    "for tweet in df.tweets:\n",
    "    clean_tweets.append(TextCleaner(tweet))\n",
    "df.insert(1, \"clean_tweets\", clean_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how our clean data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:10:45.199301Z",
     "start_time": "2020-06-10T20:10:45.186115Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to do a simple word count to see what the most frequently used words are, here's a function that plots the top words and n-grams.\n",
    "\n",
    "**N-gram** is a concept in Natural Language Processing. It simply means a sequence of n word. For example, \n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;\"SSDA\" is a uni-gram (an individual word);\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;\"good morning\" is a bi-gram (2-gram);\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;\"how are you\" is tri-gram (3-gram);\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;\"Mary had a little lamb\" is a 4-gram, and there are four bi-grams in this sentence: \"Mary had\", \"had a\", \"a little\", and \"little lamb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:15:51.831602Z",
     "start_time": "2020-06-10T20:15:51.820697Z"
    }
   },
   "outputs": [],
   "source": [
    "def PlotWords(text, n, ngram_min=1, ngram_max=1):\n",
    "    \"\"\"\n",
    "    Plot the distribution of top words in text.\n",
    "    Reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(ngram_range=(ngram_min, ngram_max), stop_words=None).fit(text)\n",
    "    #vec = CountVectorizer(ngram_range=(ngram_min, ngram_max), stop_words=\"english\").fit(text)\n",
    "    bag_of_words = vec.transform(text)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    df = pd.DataFrame(words_freq[:n], columns=[\"Topwords\", \"Count\"])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(df[\"Topwords\"], df[\"Count\"])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(\"Count\")\n",
    "    ax.set_title(\"Top {} words/ phrases\".format(n))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:15:26.178078Z",
     "start_time": "2020-06-10T20:15:25.962301Z"
    }
   },
   "outputs": [],
   "source": [
    "PlotWords(df.clean_tweets, 10, 1, 1) # consider only individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:15:36.912847Z",
     "start_time": "2020-06-10T20:15:36.675953Z"
    }
   },
   "outputs": [],
   "source": [
    "PlotWords(df.clean_tweets, 10, 2, 3) # consider only bi- and tri-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "\n",
    "## Step 2: tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is essentially chopping a phrase, a sentence, or a paragraph into smaller units, such as individual words or n-grams. These smaller units are called tokens.\n",
    "\n",
    "There are many ways to tokenize text data. We introduce two here. [This article](https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/) presents six distinct ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python's split()\n",
    "\n",
    "`split()` returns a list of strings after breaking the given string by the specified separator (only one separator at a time). It does not consider punctuation as a separate token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:18:08.038176Z",
     "start_time": "2020-06-10T20:18:08.019789Z"
    }
   },
   "outputs": [],
   "source": [
    "\"MSU leaders are meeting daily and taking all appropriate steps to ensure the health of the Spartan community.\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has a reputation for being super fast. It is written in [Cython](https://cython.org/) (C-extension for Python, which means accelerated code execution speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:19:46.538213Z",
     "start_time": "2020-06-10T20:19:46.339719Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = English()\n",
    "mytext = parser(\"MSU leaders are meeting daily and taking all appropriate steps to ensure the health of the Spartan community.\")\n",
    "tokens = []\n",
    "for token in mytext:\n",
    "    tokens.append(token.text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:46.901081Z",
     "start_time": "2020-06-10T20:35:46.891873Z"
    }
   },
   "outputs": [],
   "source": [
    "def Tokenize(text):\n",
    "    \"\"\"Break text into tokens.\"\"\"\n",
    "    parser = English()\n",
    "    parsed_text = parser(text)\n",
    "    tokens = []\n",
    "    for token in parsed_text:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        else:\n",
    "            tokens.append(token.lower_) # convert to lower cases\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:47.766669Z",
     "start_time": "2020-06-10T20:35:47.679763Z"
    }
   },
   "outputs": [],
   "source": [
    "Tokenize(\"MSU leaders are meeting daily and taking all appropriate steps to ensure the health of the Spartan community.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "\n",
    "## Step 3: word stemming/ lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of stemming and lemmatization is the same: to return a word to its root. The difference is the way they work.\n",
    "\n",
    "> **Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
    "> **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the **lemma**.\n",
    ">\n",
    ">Source: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "\n",
    "For example,\n",
    "\n",
    "|**Word**|**Stemming**|**Lemmatization**|\n",
    "|--------|------------|-----------------|\n",
    "|studying| study      |study            |\n",
    "|studies | studi      |study            |\n",
    "|was     |wa          |be               |\n",
    "\n",
    "Lemmatization is usually the preferred way of reducing related words to a common base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:49.779706Z",
     "start_time": "2020-06-10T20:35:49.774307Z"
    }
   },
   "outputs": [],
   "source": [
    "lemma = wn.morphy(\"spartans\")\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:50.380344Z",
     "start_time": "2020-06-10T20:35:50.373663Z"
    }
   },
   "outputs": [],
   "source": [
    "lemma = wn.morphy(\"is\")\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:50.922977Z",
     "start_time": "2020-06-10T20:35:50.914146Z"
    }
   },
   "outputs": [],
   "source": [
    "lemma = wn.morphy(\"are\") # if you're interested, try \"was\", what did you find?\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:51.472261Z",
     "start_time": "2020-06-10T20:35:51.466541Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(wn.morphy.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:52.032250Z",
     "start_time": "2020-06-10T20:35:52.026076Z"
    }
   },
   "outputs": [],
   "source": [
    "lemma = wn.morphy(\"are\", wn.VERB)\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it didn't do a very good job without a part-of-speech (POS) tag. However, it is impossible to manually provide appropriate POS tags for every word for large amount of text. In this workshop, let's ignore POS tags and see what we get. If you'd like to know more about word lemmatization with POS tags, check out this [article](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:53.311191Z",
     "start_time": "2020-06-10T20:35:53.303755Z"
    }
   },
   "outputs": [],
   "source": [
    "lemma = wn.morphy(\"msu\")\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:54.100403Z",
     "start_time": "2020-06-10T20:35:54.094554Z"
    }
   },
   "outputs": [],
   "source": [
    "def GetLemma(word):\n",
    "    \"\"\"Get the root of a given word.\"\"\"\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:54.863318Z",
     "start_time": "2020-06-10T20:35:54.854977Z"
    }
   },
   "outputs": [],
   "source": [
    "GetLemma(\"msu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "\n",
    "## Step 4: remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:56.366713Z",
     "start_time": "2020-06-10T20:35:56.361122Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(set(nltk.corpus.stopwords.words(\"english\"))) # check out nltk stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:57.006659Z",
     "start_time": "2020-06-10T20:35:56.998478Z"
    }
   },
   "outputs": [],
   "source": [
    "def ProcessToken(mytext):\n",
    "    \"\"\"Tokenize, remove stopwords, and get lemma of a given sentence.\"\"\"\n",
    "    en_stop = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    tokens = Tokenize(mytext)\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in en_stop:\n",
    "            clean_tokens.append(GetLemma(token))\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:35:57.965808Z",
     "start_time": "2020-06-10T20:35:57.880711Z"
    }
   },
   "outputs": [],
   "source": [
    "ProcessToken(\"MSU leaders are meeting daily and taking all appropriate steps to ensure the health of the Spartan community\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:36:11.193890Z",
     "start_time": "2020-06-10T20:35:58.920523Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_tweets = []\n",
    "for tweet in clean_tweets:\n",
    "    processed_tweets.append(ProcessToken(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:39:16.210691Z",
     "start_time": "2020-06-10T20:39:16.178166Z"
    }
   },
   "outputs": [],
   "source": [
    "#processed_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "\n",
    "## Step 5: find topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready for LDA topic modeling. We will use `gensim` for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T19:43:55.043977Z",
     "start_time": "2020-06-05T19:43:55.035235Z"
    }
   },
   "source": [
    "We need to first initialize a **dictionary**, which is a list of unique tokens in our documents (i.e. all tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:39:44.047483Z",
     "start_time": "2020-06-10T20:39:44.036818Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(processed_tweets) # initialize a dictionary\n",
    "#list(dictionary.values())\n",
    "#len(dictionary) # number of unique tokens\n",
    "#print(dictionary.token2id) # token to id map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to convert our documents to bags of words, which is referred to as **corpus** in `gensim`. Corpus is an object that contains the token id and its frequency in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:40:34.808043Z",
     "start_time": "2020-06-10T20:40:34.800493Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(t) for t in processed_tweets] # initialize a corpus\n",
    "#[i for i in corpus] # take a look at the corpus: word id and its frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need to do is to let `gensim` train a model with the dictionary and the corpus as inputs. We also need to decide how many topics there are in our tweets. Two is a good place to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:41:42.943572Z",
     "start_time": "2020-06-10T20:41:40.387663Z"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=20) # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:41:45.249611Z",
     "start_time": "2020-06-10T20:41:45.237945Z"
    }
   },
   "outputs": [],
   "source": [
    "model.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "\n",
    "## Step 6: interpret the topics and improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret the topics requires a lot of human knowledge and judgment. We need to apply our domain knowledge to check if the topics generated by the model make sense. If not, go back and find ways to improve the model. Here are a few things to experiment with:\n",
    "\n",
    "- Adjust the number of passes\n",
    "- Adjust the number of topics\n",
    "- Eliminate words that do not carry meaning by themselves, e.g. get\n",
    "- Eliminate words that are too common, e.g. msu\n",
    "- Use POS tags to improve the accuracy of lemmatization \n",
    "- Look at words that are from specific parts of speech (only nouns, only adjectives, both nouns and verbs, etc.)\n",
    "- Instead only uni-grams, consider adding bi-grams or higher order n-grams\n",
    "\n",
    "Let's try removing some words first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:47:40.723109Z",
     "start_time": "2020-06-10T20:47:40.714796Z"
    }
   },
   "outputs": [],
   "source": [
    "def ProcessToken2(mytext, remove_term=[]):\n",
    "    \"\"\"\n",
    "    Tokenize, remove stopwords, remove user-defined terms, and get lemma of a given sentence.\n",
    "\n",
    "    Input:\n",
    "    mytext -- a string;\n",
    "    remove_term -- a list words to remove (default: empty).\n",
    "    Output:\n",
    "    clean tokens.\n",
    "    \"\"\"\n",
    "    en_stop = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    remove_term = set(remove_term)\n",
    "    tokens = Tokenize(mytext)\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in remove_term:\n",
    "            if token not in en_stop:\n",
    "                clean_tokens.append(GetLemma(token))\n",
    "    final_tokens = []\n",
    "    for token in clean_tokens:\n",
    "        if token not in remove_term:\n",
    "            final_tokens.append(token)\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:47:41.583301Z",
     "start_time": "2020-06-10T20:47:41.493222Z"
    }
   },
   "outputs": [],
   "source": [
    "ProcessToken2(\"MSU leaders are meeting daily and taking all appropriate steps to ensure the health of the Spartan community\",\n",
    "              remove_term=[\"msu\", \"spartan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:48:17.203559Z",
     "start_time": "2020-06-10T20:48:05.220901Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_tweets = []\n",
    "for tweet in clean_tweets:\n",
    "    processed_tweets.append(ProcessToken2(tweet, remove_term=[\"msu\", \"spartan\", \"get\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:48:21.374829Z",
     "start_time": "2020-06-10T20:48:18.690012Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(processed_tweets)\n",
    "corpus = [dictionary.doc2bow(t) for t in processed_tweets]\n",
    "model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:48:31.509976Z",
     "start_time": "2020-06-10T20:48:31.501090Z"
    }
   },
   "outputs": [],
   "source": [
    "model.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"activity\"></a>\n",
    "\n",
    "# Activities - try it yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase passes and change the number of topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:44:58.267568Z",
     "start_time": "2020-06-07T16:44:53.132504Z"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, passes=50)\n",
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate words with extreme frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:44:58.278980Z",
     "start_time": "2020-06-07T16:44:58.272104Z"
    }
   },
   "outputs": [],
   "source": [
    "#help(dictionary.filter_extremes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:44:59.329316Z",
     "start_time": "2020-06-07T16:44:58.283834Z"
    }
   },
   "outputs": [],
   "source": [
    "#Only keep tokens that are contained in at least 5 tweets and no more than 90% of tweets\n",
    "dictionary = corpora.Dictionary(processed_tweets)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.9)\n",
    "corpus = [dictionary.doc2bow(t) for t in processed_tweets]\n",
    "model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=10)\n",
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anything else you'd like to try?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a great article on how to build better topic models by looking at **topic coherence**: [Evaluate Topic Models: Latent Dirichlet Allocation](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"7\"></a>\n",
    "\n",
    "## Optional: label documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:52:10.624257Z",
     "start_time": "2020-06-10T20:52:10.616520Z"
    }
   },
   "outputs": [],
   "source": [
    "def ShowTopics(text, model, corpus):\n",
    "    \"\"\"Show the topic distributions in each document.\"\"\"\n",
    "    topics = []\n",
    "    for i in range(len(text)):\n",
    "        topics.append(model[corpus[i]])\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:52:11.329895Z",
     "start_time": "2020-06-10T20:52:11.321220Z"
    }
   },
   "outputs": [],
   "source": [
    "def WhichTopic(text, model, corpus):\n",
    "    \"\"\"Find the most likely topic for each document.\"\"\"\n",
    "    topic = []\n",
    "    for i in range(len(text)):\n",
    "        topic_index, topic_value = max(model[corpus[i]], key=lambda item: item[1])\n",
    "        topic.append(topic_index)\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T20:52:12.339432Z",
     "start_time": "2020-06-10T20:52:12.192042Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"topics\"]=ShowTopics(processed_tweets, model, corpus)\n",
    "df[\"most_likely\"]=WhichTopic(processed_tweets, model, corpus)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"8\"></a>\n",
    "\n",
    "## Optional: visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T21:36:37.830304Z",
     "start_time": "2020-06-05T21:36:37.823428Z"
    }
   },
   "source": [
    "Note: this is extremely slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:45:07.719987Z",
     "start_time": "2020-06-07T16:44:59.504709Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"additional\"></a>\n",
    "\n",
    "# Additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T21:35:31.263053Z",
     "start_time": "2020-06-05T21:35:31.255620Z"
    }
   },
   "source": [
    "The LDA paper: [Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "\n",
    "Other topic modeling algorithms: [Topic Modeling with LSA, PLSA, LDA & lda2Vec](https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05)\n",
    "\n",
    "Comparing different tokenization methods and libraries: [How to Get Started with NLP – 6 Unique Methods to Perform Tokenization](https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/)\n",
    "\n",
    "Comparing different lemmatization methods and libraries: [Lemmatization Approaches with Examples in Python](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)\n",
    "\n",
    "Building interpretable topic models: [Evaluate Topic Models: Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)\n",
    "\n",
    "More on visualization: [pyLDAvis](https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb#topic=0&lambda=1&term=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
